{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Super Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import patsy\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import ttest_ind\n",
    "import folium\n",
    "import geocoder\n",
    "DEFAULT_LOCATION = [32.821932,-117.1509477] #center location to start the map at \n",
    "map_sd = folium.Map(location=DEFAULT_LOCATION,zoom_start=10, max_zoom=11, min_zoom=10) #constrain the zoom\n",
    "map_sd2 = folium.Map(location=DEFAULT_LOCATION,zoom_start=10, max_zoom=11, min_zoom=10) #constrain the zoom\n",
    "map_sd3 = folium.Map(location=DEFAULT_LOCATION,zoom_start=10, max_zoom=11, min_zoom=10) #constrain the zoom\n",
    "map_sd4 = folium.Map(location=DEFAULT_LOCATION,zoom_start=10, max_zoom=11, min_zoom=10) #constrain the zoom\n",
    "map_sd5 = folium.Map(location=DEFAULT_LOCATION,zoom_start=10, max_zoom=11, min_zoom=10) #constrain the zoom\n",
    "map_sd6 = folium.Map(location=DEFAULT_LOCATION,zoom_start=10, max_zoom=11, min_zoom=10) #constrain the zoom\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although most cities in America are designed in such a way that it is almost necessary to have a car to get around, today, factors like overpopulation, air and noise pollution and lack of free space constantly urge that a more efficient solution is created for people to get around for their daily needs.  Urban neighborhoods today are an intertwined mesh of commercial activity, transports and residences, and at most times, people live close by to the places they usually visit, for which having a car to get around isn’t entirely necessary, especially if there is a more efficient way to get around. However, issues like road safety, pollution, lack of bicycle lanes etc. often cause us to use our cars when there could be no apparent need, which eventually just adds to the problem, hence urging us to recognize a solution that is both more efficient towards the environment and our time, as well as has economic and commercial benefits for society.\n",
    "\n",
    "Faced with increasing issues of pollution, traffic and crowded spaces, Barcelona’s city planners created and currently implement a system where they blocked off certain urban areas from vehicle transport, and promoted commercial and community use in these areas. The results were astounding: 60% of the total streets in the city freed up space, traffic has reduced overall by 21%, 300km of new cycling lanes have been developed all around the city, commercial activity has boomed and accessibility of transit stops has increased magnificently.  \n",
    "\n",
    "Given the success of this project in Barcelona, we have created a similar model for the city of San Diego, where our analyses of the foot-traffic in various locations all over the city gives the ideal position of where a ‘Superblock’ can be placed to improve commercial activity and community engagement as well as reduce pollution. We obtained an index to represent how ‘busy' different locations are in San Diego, using three different data-sets from different local businesses, transit stations and parking meters that comprehensively cover foot-traffic of a given area, compared using geographical coordinates. After gathering this information, we recognized certain locations in San Diego that can be cordoned off to create a healthier and more efficient society, which we have geo-spatially illustrated in this project. \n",
    "\n",
    "##### Referencess:\n",
    "  ######  1)https://www.theguardian.com/cities/2016/may/17/superblocks-rescue-barcelona-spain-plan-give-streets-back-residents\n",
    "   ###### 2) https://www.walkscore.com/professional/research.php \n",
    "   ###### 3)https://www.wired.com/2017/04/brilliant-simplicity-new-yorks-new-times-square/ \n",
    "   ###### 4)http://krqe.com/2017/03/06/city-councilor-wants-wider-sidewalks-to-help-businesses-impacted-by-art/\n",
    "   ###### 5)http://www.nyc.gov/html/dot/downloads/pdf/dot-economic-benefits-of-sustainable-streets.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'Business.csv' is an offical San Diego govt dataset that contains the list of all active registered businesses in San Diego as of April 2017. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'FY2014 Ridership_Trolley_Sept2013Booking.csv' is an official San Diego govt dataset that gives us the number of people that use transit system in San Deigo as of April 2017. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "'treas_parking_payments_2017_datasd.csv' is an official San Deigo govt dataset that gives us the information of all the parking meter transactions upto April 2017. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning / Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File Business.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-ee6187ad5bbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Business.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbusiness_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/raghav/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/raghav/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/raghav/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/raghav/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    921\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 923\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    924\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/raghav/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1388\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:4184)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:8449)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File Business.csv does not exist"
     ]
    }
   ],
   "source": [
    "#Importing the original file for active business data in SD\n",
    "\n",
    "fname = \"Business.csv\"\n",
    "business_df = pd.read_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importing the clean file for train stops data in SD\n",
    "\n",
    "fname1 = \"Stop_Counts.csv\"\n",
    "stops_df = pd.read_csv(fname1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importing the clean file for parking meter transactions data in SD\n",
    "\n",
    "fname2 = \"Parking_Counts.csv\"\n",
    "parking_df = pd.read_csv(fname2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Active Businesses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Removing unneccesary columns and renaming to layman terms, filtering all business into places that serve food/drinks\n",
    "\n",
    "business_df = business_df[['doing_bus_as_name','zip','naics_description','lat','lon']]\n",
    "business_df = business_df.loc[(business_df['naics_description'] == 'full-service restaurants') |\n",
    "               (business_df['naics_description'] == 'cafeterias') | \n",
    "               (business_df['naics_description'] == 'food services & drinking places') |\n",
    "               (business_df['naics_description'] == 'limited-service eating places') |\n",
    "               (business_df['naics_description'] == 'limited-service restaurants')  |\n",
    "               (business_df['naics_description'] == 'mobile food services') |\n",
    "               (business_df['naics_description'] == 'drinking places (alcoholic beverages)') |\n",
    "               (business_df['naics_description'] == 'snack & nonalcoholic beverage bars')]\n",
    "business_df.rename(columns = {'doing_bus_as_name':'Business title','naics_description':'Type of Place'}, inplace=True)\n",
    "business_df = business_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Adding the 5 columns for the different time brackets\n",
    "\n",
    "business_df.insert(2,'AM early','Null')\n",
    "business_df.insert(3,'AM peak','Null')\n",
    "business_df.insert(4,'Mid-day','Null')\n",
    "business_df.insert(5,'PM peak','Null')\n",
    "business_df.insert(6,'PM late','Null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generalising the different categories into 3 categories: Only Food, Food & Drinks, Only Drinks\n",
    "\n",
    "business_df.loc[(business_df['Type of Place'] == 'mobile food services') |\n",
    "                (business_df['Type of Place'] == 'cafeterias') |\n",
    "                (business_df['Type of Place'] == 'snack & nonalcoholic beverage bars') |\n",
    "                (business_df['Type of Place'] == 'limited-service eating places') |\n",
    "                (business_df['Type of Place'] == 'limited-service restaurants') \n",
    "                , 'Type of Place'] = 'Only Food'\n",
    "\n",
    "business_df.loc[(business_df['Type of Place'] == 'full-service restaurants') \n",
    "                , 'Type of Place'] = 'Food & Drinks'\n",
    "\n",
    "business_df.loc[(business_df['Type of Place'] == 'drinking places (alcoholic beverages)') |\n",
    "                (business_df['Type of Place'] == 'food services & drinking places') \n",
    "                , 'Type of Place'] = 'Only Drinks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Assigning values to different time brackets by assuming foot traffic according to the type of place \n",
    "\n",
    "business_df.loc[(business_df['Type of Place'] == 'Only Food'), \n",
    "                ('AM early','AM peak','Mid-day','PM peak','PM late')] = ('3','25','20','18','5')\n",
    "business_df.loc[(business_df['Type of Place'] == 'Food & Drinks'),\n",
    "                ('AM early','AM peak','Mid-day','PM peak','PM late')] = ('7','28','25','35','25')\n",
    "business_df.loc[(business_df['Type of Place'] == 'Only Drinks'),\n",
    "                ('AM early','AM peak','Mid-day','PM peak','PM late')] = ('27','6','3','27','30') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Assigning a total score to all three categories (summ of scores of all time brackets)\n",
    "\n",
    "business_df.loc[(business_df['Type of Place'] == 'Only Food'), \n",
    "                'Total Score'] = '71'\n",
    "business_df.loc[(business_df['Type of Place'] == 'Food & Drinks'),\n",
    "                'Total Score'] = '120'\n",
    "business_df.loc[(business_df['Type of Place'] == 'Only Drinks'),\n",
    "                'Total Score'] = '93'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "business_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "business_df.to_csv('Restaurant_Counts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transit Stops Dataset\n",
    "The following cells take the original dataset and clean it so that we get the cleaned data, NOT MEANT TO BE RUN since we already have the cleaned dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#Code which we used to clean the original dataset which is way too big for Github \n",
    "\n",
    "trips = pd.read_csv('FY2014 Ridership_Trolley_Sept2013Booking.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Removing unnecessary columns: all we need is stop_id, passengers getting off on the station \n",
    "# and time_arrival to get the time split for the number of people getting off at a station at a time_period\n",
    "\n",
    "trips = trips[['STOP_ID', 'PASSENGERS_OFF', 'TIME_ACTUAL_ARRIVE']]\n",
    "trips.columns = ['stop_id', 'count', 'time']\n",
    "trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### clean the data, removing any na rows we will also remove all rows that have value 0\n",
    "\n",
    "trips.dropna(how='any')\n",
    "trips = trips[trips['count'] != 0]\n",
    "trips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Using stop_id we can connect with lat/long. We will be grouping by pole_id to create a table with \n",
    "# the following columns: (stop_id, latitude, longitude, count_am_early, count_am_peak, count_midday, \n",
    "# count_pm_early, count_pm_late, count_daily)\n",
    "\n",
    "#### Getting location data for all the stops using another dataset\n",
    "stop_locs = pd.read_csv('FY2014 Ridership_Trolley_Sept2013_Stops.csv')\n",
    "stop_locs = stop_locs[['STOP_ID', 'LAT', 'LON']]\n",
    "stop_locs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "stop_locs.columns = ['stop_id', 'latitude', 'longitude']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "stops_df = trips.merge(stop_locs, how='left')\n",
    "stops_df = stops_df.dropna(how='any')\n",
    "stops_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### grouping by stop_id and aggregating over the count values\n",
    "\n",
    "stop_counts = stops_df.groupby(['stop_id']).agg('count')\n",
    "\n",
    "##### remove unnecessary columns\n",
    "\n",
    "stop_counts = stop_counts['time'] \n",
    "stop_counts = stop_counts.to_frame()\n",
    "stop_counts['stop_id'] = stop_counts.index\n",
    "stop_counts = stop_counts.merge(stop_locs, how='left')\n",
    "stop_counts.columns = ['total_count', 'stop_id', 'latitude', 'longitude']\n",
    "stop_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### We need to now find the number of days in the transactions dataset\n",
    "##### We will be using this in order to get the count of transactions PER DAY\n",
    "\n",
    "stops_df['time'] = pd.to_datetime(stops_df['time'])\n",
    "dates = stops_df['time']\n",
    "am_early_d = {}\n",
    "am_peak_d = {}\n",
    "midday_d = {}\n",
    "pm_peak_d = {}\n",
    "pm_late_d = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The time ranges for which we have split the transit ridership data are:\n",
    "AM_Early = 12AM-6AM\n",
    "AM_Late = 6AM-9AM\n",
    "..\n",
    "..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Classify the time slot based on the times during the day\n",
    "def classify(x): \n",
    "    hour = x.time().hour\n",
    "    if hour <=6:\n",
    "        return 'am_early'\n",
    "    elif hour <=9:\n",
    "        return 'am_peak'\n",
    "    elif hour <=14:\n",
    "        return 'midday'\n",
    "    elif hour <=19:\n",
    "        return 'pm_peak'\n",
    "    else:\n",
    "        return 'pm_late'\n",
    "stops_df['time_slot'] = stops_df['time'].apply(classify)\n",
    "stops_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Merging the two datasets by key value and column name\n",
    "\n",
    "def checkSeriesColumn(s, col):\n",
    "    val = False\n",
    "    for row in s.keys().to_series().str.contains(col): \n",
    "        if(row == True):\n",
    "            val = True\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Setting the values of the transactions for each time period during the day\n",
    "def set_temporal_counts(p_id):\n",
    "    v_counts = stops_df.loc[stops_df['stop_id'] == p_id]['time_slot'].value_counts(dropna=False)\n",
    "    stop_counts.loc[stop_counts['stop_id'] == p_id,'am_early'] = 0 if not checkSeriesColumn(v_counts, 'am_early') else v_counts['am_early']\n",
    "    stop_counts.loc[stop_counts['stop_id'] == p_id,'am_peak'] =  0 if not checkSeriesColumn(v_counts, 'am_peak') else v_counts['am_peak']\n",
    "    stop_counts.loc[stop_counts['stop_id'] == p_id,'midday'] =  0 if not checkSeriesColumn(v_counts, 'midday') else v_counts['midday']\n",
    "    stop_counts.loc[stop_counts['stop_id'] == p_id,'pm_peak'] =  0 if not checkSeriesColumn(v_counts, 'pm_peak') else v_counts['pm_peak']\n",
    "    stop_counts.loc[stop_counts['stop_id'] == p_id,'pm_late'] = 0 if not checkSeriesColumn(v_counts, 'pm_late') else v_counts['pm_late']\n",
    "\n",
    "stop_counts['stop_id'].apply(set_temporal_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "stop_counts.to_csv('Stop_Counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cleaned dataset for the transit spots\n",
    "stops_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parking meters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Importing data and initial cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#importing original dataset\n",
    "parking = pd.read_csv('treas_parking_payments_2017_datasd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing unnecessary columns: all we need is pole_id, time_start and meter_expire\n",
    "parking = parking.drop(['uuid'], axis=1)\n",
    "parking = parking.drop(['trans_amt'], axis=1)\n",
    "parking = parking.drop(['pay_method'], axis=1)\n",
    "parking = parking.drop(['meter_type'], axis=1)\n",
    "parking = parking.drop(['meter_expire'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#clean the data, removing any na rows \n",
    "parking.dropna(how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Connecting with other dataset (with lat/long pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2.) using pole_id we can connect with lat/long. We will be grouping by pole_id to create\n",
    "#     a table with the following columns:\n",
    "#     (pole_id, latitude, longitude, count_am_early, count_am_peak, \n",
    "#        count_midday, count_pm_early, count_pm_late, count_daily)\n",
    "# NOTE: we will need to decide whether to use raw numbers or averages of counts per section per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "park_loc = pd.read_csv('treas_parking_meters_loc_datasd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "park_loc = park_loc[['pole', 'longitude', 'latitude']]\n",
    "park_loc.columns = ['pole_id', 'longitude', 'latitude']\n",
    "park_loc = park_loc.dropna(how='any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "park_df = parking.merge(park_loc, how='left')\n",
    "park_df = park_df.dropna(how='any')\n",
    "park_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "park_counts = park_df.groupby(['pole_id']).agg('count')\n",
    "park_counts = park_counts['trans_start'] #remove unnecessary columns\n",
    "park_counts = park_counts.to_frame()\n",
    "park_counts['pole_id'] = park_counts.index\n",
    "park_counts = park_counts.merge(park_loc, how='left')\n",
    "park_counts.columns = ['total_count', 'pole_id', 'longitude', 'latitude']\n",
    "park_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we need to now find the number of days in the transactions dataset\n",
    "# we will be using this in order to get the count of transactions PER DAY\n",
    "park_df['trans_start'] = pd.to_datetime(park_df['trans_start'])\n",
    "dates = park_df['trans_start']\n",
    "am_early_d = {}\n",
    "am_peak_d = {}\n",
    "midday_d = {}\n",
    "pm_peak_d = {}\n",
    "pm_late_d = {}\n",
    "\n",
    "def classify(x): \n",
    "    hour = x.time().hour\n",
    "    if hour <=6:\n",
    "        return 'am_early'\n",
    "    elif hour <=9:\n",
    "        return 'am_peak'\n",
    "    elif hour <=14:\n",
    "        return 'midday'\n",
    "    elif hour <=19:\n",
    "        return 'pm_peak'\n",
    "    else:\n",
    "        return 'pm_late'\n",
    "park_df['time_slot'] = park_df['trans_start'].apply(classify)\n",
    "park_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def checkSeriesColumn(s, col):\n",
    "    val = False\n",
    "    for row in s.keys().to_series().str.contains(col): \n",
    "        if(row == True):\n",
    "            val = True\n",
    "    return val\n",
    "def set_temporal_counts(p_id):\n",
    "    v_counts = park_df.loc[park_df['pole_id'] == p_id]['time_slot'].value_counts(dropna=False)\n",
    "    park_counts.loc[park_counts['pole_id'] == p_id,'am_early'] = 0 if not checkSeriesColumn(v_counts, 'am_early') else v_counts['am_early']\n",
    "    park_counts.loc[park_counts['pole_id'] == p_id,'am_peak'] =  0 if not checkSeriesColumn(v_counts, 'am_peak') else v_counts['am_peak']\n",
    "    park_counts.loc[park_counts['pole_id'] == p_id,'midday'] =  0 if not checkSeriesColumn(v_counts, 'midday') else v_counts['midday']\n",
    "    park_counts.loc[park_counts['pole_id'] == p_id,'pm_peak'] =  0 if not checkSeriesColumn(v_counts, 'pm_peak') else v_counts['pm_peak']\n",
    "    park_counts.loc[park_counts['pole_id'] == p_id,'pm_late'] = 0 if not checkSeriesColumn(v_counts, 'pm_late') else v_counts['pm_late']\n",
    "#     park_counts.loc[park_counts['pole_id'] == ]\n",
    "#      park_counts.loc[park_counts['pole_id'] == p_id,'am_early'] = 0 if not checkSeriesColumn(v_counts, 'am_early') else v_counts['am_early']\n",
    "#      park_counts.loc[park_counts['pole_id'] == p_id,'am_peak'] =  0 if not checkSeriesColumn(v_counts, 'am_peak') else v_counts['am_peak']\n",
    "#     park_counts.loc[park_counts['pole_id'] == p_id,'midday'] =  0 if not checkSeriesColumn(v_counts, 'midday') else v_counts['midday']\n",
    "#     park_counts.loc[park_counts['pole_id'] == p_id,'pm_peak'] =  0 if not checkSeriesColumn(v_counts, 'pm_peak') else v_counts['pm_peak']\n",
    "#     park_counts.loc[park_counts['pole_id'] == p_id,'pm_late'] = 0 if not checkSeriesColumn(v_counts, 'pm_late') else v_counts['pm_late']\n",
    "\n",
    "\n",
    "park_counts['pole_id'].apply(set_temporal_counts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# v_counts = park_df.loc[park_df['pole_id'] == 'N-1003']['time_slot'].value_counts(dropna=False)\n",
    "# park_counts.loc[park_counts['pole_id'] == 'N-1003','am_early'] = 0 if not checkSeriesColumn(v_counts, 'am_early') else v_counts['am_early']\n",
    "# park_counts.loc[park_counts['pole_id'] == 'N-1003','am_peak'] =  0 if not checkSeriesColumn(v_counts, 'am_peak') else v_counts['am_peak']\n",
    "# park_counts.loc[park_counts['pole_id'] == 'N-1003','midday'] =  0 if not checkSeriesColumn(v_counts, 'midday') else v_counts['midday']\n",
    "# park_counts.loc[park_counts['pole_id'] == 'N-1003','pm_peak'] =  0 if not checkSeriesColumn(v_counts, 'pm_peak') else v_counts['pm_peak']\n",
    "# park_counts.loc[park_counts['pole_id'] == 'N-1003','pm_late'] = 0 if not checkSeriesColumn(v_counts, 'pm_late') else v_counts['pm_late']\n",
    "# park_counts.loc[park_counts['pole_id'] == 'N-1003']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_date = dates.max()\n",
    "min_date = dates.min()\n",
    "days_elapsed = (max_date - min_date).days + 1 #to round off this number\n",
    "days_elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'park_counts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-29bd3981fcf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpark_counts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Parking_Counts.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'park_counts' is not defined"
     ]
    }
   ],
   "source": [
    "park_counts.to_csv(\"Parking_Counts.csv\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's now get average counts of parked vehicles\n",
    "# dividing by total days to give per daily expected counts \n",
    "park_counts['total_count'] = park_counts['total_count'].apply(lambda x: x / days_elapsed)\n",
    "park_counts['am_early'] = park_counts['am_early'].apply(lambda x: x / days_elapsed)\n",
    "park_counts['am_peak'] = park_counts['am_peak'].apply(lambda x: x / days_elapsed)\n",
    "park_counts['midday'] = park_counts['midday'].apply(lambda x: x / days_elapsed)\n",
    "park_counts['pm_peak'] = park_counts['pm_peak'].apply(lambda x: x / days_elapsed)\n",
    "park_counts['pm_late'] = park_counts['pm_late'].apply(lambda x: x / days_elapsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we then multiply this amount by the mean number of people per vehicle\n",
    "# as per https://www.rita.dot.gov/bts/sites/rita.dot.gov.bts/files/publications/highlights_of_the_2001_national_household_travel_survey/html/table_a15.html\n",
    "ppl_per_vehicle = 1.63\n",
    "park_counts['total_count'] = park_counts['total_count'].apply(lambda x: x * ppl_per_vehicle)\n",
    "park_counts['am_early'] = park_counts['am_early'].apply(lambda x: x * ppl_per_vehicle)\n",
    "park_counts['am_peak'] = park_counts['am_peak'].apply(lambda x: x * ppl_per_vehicle)\n",
    "park_counts['midday'] = park_counts['midday'].apply(lambda x:  x * ppl_per_vehicle)\n",
    "park_counts['pm_peak'] = park_counts['pm_peak'].apply(lambda x:  x * ppl_per_vehicle)\n",
    "park_counts['pm_late'] = park_counts['pm_late'].apply(lambda x:  x * ppl_per_vehicle)\n",
    "park_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "park_counts = pd.read_csv(\"Parking_Counts.csv\");\n",
    "####Rough Calculations for algorithm\n",
    "long_max = park_counts['longitude'].max()\n",
    "long_min = park_counts['longitude'].min()\n",
    "lat_max = park_counts['latitude'].max()\n",
    "lat_min = park_counts['latitude'].min()\n",
    "lat_dif = lat_max - lat_min\n",
    "long_dif = long_max - long_min\n",
    "NUMBER_BLOCKS_ROOT = 10 #this means 100 blocks 10x10\n",
    "lat_gap = lat_dif / NUMBER_BLOCKS_ROOT\n",
    "long_gap = long_dif / NUMBER_BLOCKS_ROOT\n",
    "\n",
    "def classify_blocks(s):\n",
    "    park_counts.loc[park_counts['pole_id'] == s,'row'] =  (park_counts.loc[park_counts['pole_id'] == s,'latitude'] - lat_min) // lat_gap\n",
    "    park_counts.loc[park_counts['pole_id'] == s,'col'] = (park_counts.loc[park_counts['pole_id'] == s,'longitude'] - long_min) // long_gap\n",
    "\n",
    "park_counts['pole_id'].apply(classify_blocks)\n",
    "\n",
    "park_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(long_max)\n",
    "print(long_min)\n",
    "print(long_dif)\n",
    "print(lat_max)\n",
    "print(lat_min)\n",
    "print(lat_dif)\n",
    "park_counts = park_counts.loc[park_counts['longitude'] != -180.0 ] #remove the outlier\n",
    "long_min = park_counts['longitude'].min()\n",
    "long_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "park_counts.to_csv(\"Parking_Counts_grid.csv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis and Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "park_counts = pd.read_csv('Parking_Counts.csv')\n",
    "park_counts = park_counts.loc[park_counts['longitude'] != -180.0 ] #remove the outlier\n",
    "park_counts.rename(columns={'pole_id':'id'}, inplace= True)\n",
    "park_counts['type'] = 'parking'\n",
    "del park_counts['Unnamed: 0']\n",
    "park_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "restaurants = pd.read_csv('Restaurant_Counts.csv')\n",
    "restaurants.columns = ['id', 'title', 'zip', 'am_early', 'am_peak', 'midday', 'pm_peak', 'pm_late', 'type', 'latitude', 'longitude', 'total_count']\n",
    "restaurants['total_count'] = restaurants['total_count'].apply( lambda x : x * 2/3)\n",
    "restaurants['am_peak'] = restaurants['am_peak'].apply( lambda x : x * 2/3)\n",
    "restaurants['am_early'] = restaurants['am_early'].apply( lambda x : x * 2/3)\n",
    "restaurants['midday'] = restaurants['midday'].apply( lambda x : x * 2/3)\n",
    "restaurants['pm_peak'] = restaurants['pm_peak'].apply( lambda x : x * 2/3)\n",
    "restaurants['pm_late'] = restaurants['pm_late'].apply( lambda x : x * 2/3)\n",
    "restaurants['id'] = restaurants['id'].apply(lambda x : ('R-' + str(x) ))\n",
    "restaurants = restaurants[['id','type','title', 'am_early', 'am_peak', 'midday', 'pm_peak', 'pm_late', 'total_count','latitude', 'longitude']]\n",
    "\n",
    "restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transit = pd.read_csv('Stop_Counts.csv')\n",
    "transit.rename(columns={'stop_id':'id'}, inplace= True)\n",
    "transit['type'] = 'transit_stop'\n",
    "del transit['Unnamed: 0'] \n",
    "transit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "location_counts = restaurants.append(park_counts).append(transit)\n",
    "# Setting a new index\n",
    "#location_counts.index = idx # new ad hoc index\n",
    "# location_counts.index = range(len(location_counts)) # set with list\n",
    "# location_counts = location_counts.reset_index() # replace old w new\n",
    "# location_counts.rename(columns={'index':'idx'}, inplace= True)\n",
    "# location_counts.rename(columns={'id':'type_id'}, inplace= True)\n",
    "# del location_counts['Unnamed: 0']\n",
    "location_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "long_max = location_counts['longitude'].max()\n",
    "long_min = location_counts['longitude'].min()\n",
    "lat_max = location_counts['latitude'].max()\n",
    "lat_min = location_counts['latitude'].min()\n",
    "lat_dif = lat_max - lat_min\n",
    "long_dif = long_max - long_min\n",
    "NUMBER_BLOCKS_ROOT = 10 #this means 100 blocks 10x10\n",
    "lat_gap = lat_dif / NUMBER_BLOCKS_ROOT\n",
    "long_gap = long_dif / NUMBER_BLOCKS_ROOT\n",
    "\n",
    "def classify_blocks(s):\n",
    "    location_counts.loc[location_counts['id'] == s,'row'] =  (location_counts.loc[location_counts['id'] == s,'latitude'] - lat_min) // lat_gap\n",
    "    location_counts.loc[location_counts['id'] == s,'col'] = (location_counts.loc[location_counts['id'] == s,'longitude'] - long_min) // long_gap\n",
    "\n",
    "location_counts['id'].apply(classify_blocks)\n",
    "\n",
    "location_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(location_counts['row'].min())\n",
    "print(location_counts['row'].max())\n",
    "print(location_counts['row'].mean())\n",
    "print(location_counts['row'].std())\n",
    "print(location_counts['col'].min())\n",
    "print(location_counts['col'].max())\n",
    "print(location_counts['col'].mean())\n",
    "print(location_counts['col'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid = []\n",
    "for i in range(NUMBER_BLOCKS_ROOT):\n",
    "    for j in range(NUMBER_BLOCKS_ROOT):\n",
    "        grid.append([i, j, (lat_min+ i*lat_gap), (long_min + j*long_gap)])    \n",
    "grid = pd.DataFrame(grid)\n",
    "grid.columns = ['row','col','lat','lon']\n",
    "grid\n",
    "#grid.loc[(grid['row']==142) & (grid['col']==378)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "location_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location_counts.to_csv('Location_grid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_loc_counts = location_counts[['total_count','row','col']]\n",
    "amearly_loc_counts = location_counts[['am_early','row','col']]\n",
    "ampeak_loc_counts = location_counts[['am_peak','row','col']]\n",
    "midday_loc_counts = location_counts[['midday','row','col']]\n",
    "pmlate_loc_counts = location_counts[['pm_late','row','col']]\n",
    "pmpeak_loc_counts = location_counts[['pm_peak','row','col']]\n",
    "total_loc_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weighted_grid = total_loc_counts.groupby(['row','col']).sum().sort_values(by='total_count',ascending=False)\n",
    "weighted_grid2 = amearly_loc_counts.groupby(['row','col']).sum().sort_values(by='am_early',ascending=False)\n",
    "weighted_grid3 = ampeak_loc_counts.groupby(['row','col']).sum().sort_values(by='am_peak',ascending=False)\n",
    "weighted_grid4 = midday_loc_counts.groupby(['row','col']).sum().sort_values(by='midday',ascending=False)\n",
    "weighted_grid5 = pmlate_loc_counts.groupby(['row','col']).sum().sort_values(by='pm_late',ascending=False)\n",
    "weighted_grid6 = pmpeak_loc_counts.groupby(['row','col']).sum().sort_values(by='pm_peak',ascending=False)\n",
    "weighted_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# merge these two dataframes where the values in grid \n",
    "#weighted_grid = pd.merge(weighted_grid, grid, on=['row')\n",
    "big_grid = pd.merge(grid, \n",
    "                weighted_grid.reset_index(), \n",
    "                left_on=['row','col'], \n",
    "                right_on=['row','col'], \n",
    "                how='left')\n",
    "#big_grid.loc[(big_grid['row']==142) & (big_grid['col']==378)]\n",
    "big_grid.fillna(0.0)\n",
    "\n",
    "big_grid2 = pd.merge(grid, \n",
    "                weighted_grid2.reset_index(), \n",
    "                left_on=['row','col'], \n",
    "                right_on=['row','col'], \n",
    "                how='left')\n",
    "#big_grid.loc[(big_grid['row']==142) & (big_grid['col']==378)]\n",
    "big_grid2.fillna(0.0)\n",
    "\n",
    "big_grid3 = pd.merge(grid, \n",
    "                weighted_grid3.reset_index(), \n",
    "                left_on=['row','col'], \n",
    "                right_on=['row','col'], \n",
    "                how='left')\n",
    "#big_grid.loc[(big_grid['row']==142) & (big_grid['col']==378)]\n",
    "big_grid3.fillna(0.0)\n",
    "\n",
    "big_grid4 = pd.merge(grid, \n",
    "                weighted_grid4.reset_index(), \n",
    "                left_on=['row','col'], \n",
    "                right_on=['row','col'], \n",
    "                how='left')\n",
    "#big_grid.loc[(big_grid['row']==142) & (big_grid['col']==378)]\n",
    "big_grid4.fillna(0.0)\n",
    "\n",
    "big_grid5 = pd.merge(grid, \n",
    "                weighted_grid5.reset_index(), \n",
    "                left_on=['row','col'], \n",
    "                right_on=['row','col'], \n",
    "                how='left')\n",
    "#big_grid.loc[(big_grid['row']==142) & (big_grid['col']==378)]\n",
    "big_grid5.fillna(0.0)\n",
    "\n",
    "big_grid6 = pd.merge(grid, \n",
    "                weighted_grid6.reset_index(), \n",
    "                left_on=['row','col'], \n",
    "                right_on=['row','col'], \n",
    "                how='left')\n",
    "#big_grid.loc[(big_grid['row']==142) & (big_grid['col']==378)]\n",
    "big_grid6.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "big_grid['total_count'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_grid2['am_early'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_grid3['am_peak'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_grid4['midday'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_grid5['pm_late'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_grid6['pm_peak'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_grid2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_grid3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_grid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_grid5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_grid6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "big_grid.to_csv('Heat_Map1.csv')\n",
    "big_grid2.to_csv('Heat_Map_am_early.csv')\n",
    "big_grid3.to_csv('Heat_Map_am_peak.csv')\n",
    "big_grid4.to_csv('Heat_Map_midday.csv')\n",
    "big_grid5.to_csv('Heat_Map_pm_late.csv')\n",
    "big_grid6.to_csv('Heat_Map_pm_peak.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grid = pd.read_csv('Heat_Map.csv')\n",
    "grid2 = pd.read_csv('AM_Early_Heat_Map.csv')\n",
    "grid3 = pd.read_csv('AM_Peak_Heat_Map.csv')\n",
    "grid4 = pd.read_csv('Midday_Heat_Map.csv')\n",
    "grid5 = pd.read_csv('PM_Late_Heat_Map.csv')\n",
    "grid6 = pd.read_csv('PM_Peak_Heat_Map.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#print values as strings or empty str if they are null\n",
    "def null_print(val):\n",
    "    if(val is not None):\n",
    "        return str(val)\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "#get_addresses\n",
    "def get_address(loc):\n",
    "    geo = geocoder.google(loc, method='reverse')\n",
    "    if(geo is not None):\n",
    "        address = null_print(geo.housenumber) + ' ' + null_print(geo.street) + ' ' + null_print(geo.city) + ' ' + null_print(geo.postal)\n",
    "    else: \n",
    "        address = \"Unknown\"\n",
    "    return address\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weight_feature = 'total_count'#change this for different timeslots\n",
    "\n",
    "\n",
    "#stats to use for classification\n",
    "mean = grid[weight_feature].astype(float).mean()\n",
    "std = grid[weight_feature].astype(float).std()\n",
    "\n",
    "#convert to floats for folium to read these\n",
    "grid['lat'] = grid['lat'].astype(float)\n",
    "grid['lon'] = grid['lon'].astype(float)\n",
    "\n",
    "#color constants for viz\n",
    "COLOR_LOW = '#50f442'\n",
    "COLOR_MILD = '#fff375'\n",
    "COLOR_MEDIUM = '#ffa154'\n",
    "COLOR_HIGH = '#ff5454'\n",
    "COLOR_MAX = '#a43dff'\n",
    "\n",
    "#depending on the weight we assign graded colors\n",
    "def weight_to_color(w):\n",
    "    if(w < mean - std):\n",
    "        return COLOR_LOW\n",
    "    elif (w < mean):\n",
    "        return COLOR_MILD\n",
    "    elif (w < mean + std):\n",
    "        return COLOR_MEDIUM\n",
    "    elif (w < (mean + 2 * std)):\n",
    "        return COLOR_HIGH\n",
    "    else:\n",
    "        return COLOR_MAX\n",
    "\n",
    "\n",
    "for index, row in grid.iterrows():\n",
    "    lat = np.asscalar(row['lat'])\n",
    "    lon = np.asscalar(row['lon'])\n",
    "    loc = [lat,lon]\n",
    "    if(not math.isnan(lon) and not math.isnan(lat)):\n",
    "        weight = np.asscalar(row[weight_feature])\n",
    "        color = weight_to_color(weight)\n",
    "        address = get_address(loc)\n",
    "        folium.RegularPolygonMarker(loc, popup=address, fill_color=color, rotation = -45, number_of_sides=4, radius=10, fill_opacity = 0.66).add_to(map_sd)\n",
    "\n",
    "map_sd.save(\"total_count.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weight_feature = 'am_early'#change this for different timeslots\n",
    "\n",
    "\n",
    "#stats to use for classification\n",
    "mean = grid2[weight_feature].astype(float).mean()\n",
    "std = grid2[weight_feature].astype(float).std()\n",
    "\n",
    "#convert to floats for folium to read these\n",
    "grid2['lat'] = grid2['lat'].astype(float)\n",
    "grid2['lon'] = grid2['lon'].astype(float)\n",
    "\n",
    "#color constants for viz\n",
    "COLOR_LOW = '#50f442'\n",
    "COLOR_MILD = '#fff375'\n",
    "COLOR_MEDIUM = '#ffa154'\n",
    "COLOR_HIGH = '#ff5454'\n",
    "COLOR_MAX = '#a43dff'\n",
    "\n",
    "#depending on the weight we assign graded colors\n",
    "def weight_to_color(w):\n",
    "    if(w < mean - std):\n",
    "        return COLOR_LOW\n",
    "    elif (w < mean):\n",
    "        return COLOR_MILD\n",
    "    elif (w < mean + std):\n",
    "        return COLOR_MEDIUM\n",
    "    elif (w < (mean + 2 * std)):\n",
    "        return COLOR_HIGH\n",
    "    else:\n",
    "        return COLOR_MAX\n",
    "\n",
    "\n",
    "for index, row in grid2.iterrows():\n",
    "    lat = np.asscalar(row['lat'])\n",
    "    lon = np.asscalar(row['lon'])\n",
    "    loc = [lat,lon]\n",
    "    if(not math.isnan(lon) and not math.isnan(lat)):\n",
    "        weight = np.asscalar(row[weight_feature])\n",
    "        color = weight_to_color(weight)\n",
    "        address = get_address(loc)\n",
    "        folium.RegularPolygonMarker(loc, popup=address, fill_color=color, rotation = -45, number_of_sides=4, radius=10, fill_opacity = 0.66).add_to(map_sd2)\n",
    "\n",
    "map_sd2.save(\"am_early.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_feature = 'am_peak'#change this for different timeslots\n",
    "\n",
    "\n",
    "#stats to use for classification\n",
    "mean = grid3[weight_feature].astype(float).mean()\n",
    "std = grid3[weight_feature].astype(float).std()\n",
    "\n",
    "#convert to floats for folium to read these\n",
    "grid3['lat'] = grid3['lat'].astype(float)\n",
    "grid3['lon'] = grid3['lon'].astype(float)\n",
    "\n",
    "#color constants for viz\n",
    "COLOR_LOW = '#50f442'\n",
    "COLOR_MILD = '#fff375'\n",
    "COLOR_MEDIUM = '#ffa154'\n",
    "COLOR_HIGH = '#ff5454'\n",
    "COLOR_MAX = '#a43dff'\n",
    "\n",
    "#depending on the weight we assign graded colors\n",
    "def weight_to_color(w):\n",
    "    if(w < mean - std):\n",
    "        return COLOR_LOW\n",
    "    elif (w < mean):\n",
    "        return COLOR_MILD\n",
    "    elif (w < mean + std):\n",
    "        return COLOR_MEDIUM\n",
    "    elif (w < (mean + 2 * std)):\n",
    "        return COLOR_HIGH\n",
    "    else:\n",
    "        return COLOR_MAX\n",
    "\n",
    "\n",
    "for index, row in grid3.iterrows():\n",
    "    lat = np.asscalar(row['lat'])\n",
    "    lon = np.asscalar(row['lon'])\n",
    "    loc = [lat,lon]\n",
    "    if(not math.isnan(lon) and not math.isnan(lat)):\n",
    "        weight = np.asscalar(row[weight_feature])\n",
    "        color = weight_to_color(weight)\n",
    "        address = get_address(loc)\n",
    "        folium.RegularPolygonMarker(loc, popup=address, fill_color=color, rotation = -45, number_of_sides=4, radius=10, fill_opacity = 0.66).add_to(map_sd3)\n",
    "\n",
    "map_sd3.save(\"am_peak.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_feature = 'midday'#change this for different timeslots\n",
    "\n",
    "\n",
    "#stats to use for classification\n",
    "mean = grid4[weight_feature].astype(float).mean()\n",
    "std = grid4[weight_feature].astype(float).std()\n",
    "\n",
    "#convert to floats for folium to read these\n",
    "grid4['lat'] = grid4['lat'].astype(float)\n",
    "grid4['lon'] = grid4['lon'].astype(float)\n",
    "\n",
    "#color constants for viz\n",
    "COLOR_LOW = '#50f442'\n",
    "COLOR_MILD = '#fff375'\n",
    "COLOR_MEDIUM = '#ffa154'\n",
    "COLOR_HIGH = '#ff5454'\n",
    "COLOR_MAX = '#a43dff'\n",
    "\n",
    "#depending on the weight we assign graded colors\n",
    "def weight_to_color(w):\n",
    "    if(w < mean - std):\n",
    "        return COLOR_LOW\n",
    "    elif (w < mean):\n",
    "        return COLOR_MILD\n",
    "    elif (w < mean + std):\n",
    "        return COLOR_MEDIUM\n",
    "    elif (w < (mean + 2 * std)):\n",
    "        return COLOR_HIGH\n",
    "    else:\n",
    "        return COLOR_MAX\n",
    "\n",
    "\n",
    "for index, row in grid4.iterrows():\n",
    "    lat = np.asscalar(row['lat'])\n",
    "    lon = np.asscalar(row['lon'])\n",
    "    loc = [lat,lon]\n",
    "    if(not math.isnan(lon) and not math.isnan(lat)):\n",
    "        weight = np.asscalar(row[weight_feature])\n",
    "        color = weight_to_color(weight)\n",
    "        address = get_address(loc)\n",
    "        folium.RegularPolygonMarker(loc, popup=address, fill_color=color, rotation = -45, number_of_sides=4, radius=10, fill_opacity = 0.66).add_to(map_sd4)\n",
    "\n",
    "map_sd4.save(\"midday.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_feature = 'pm_late'#change this for different timeslots\n",
    "\n",
    "\n",
    "#stats to use for classification\n",
    "mean = grid5[weight_feature].astype(float).mean()\n",
    "std = grid5[weight_feature].astype(float).std()\n",
    "\n",
    "#convert to floats for folium to read these\n",
    "grid5['lat'] = grid5['lat'].astype(float)\n",
    "grid5['lon'] = grid5['lon'].astype(float)\n",
    "\n",
    "#color constants for viz\n",
    "COLOR_LOW = '#50f442'\n",
    "COLOR_MILD = '#fff375'\n",
    "COLOR_MEDIUM = '#ffa154'\n",
    "COLOR_HIGH = '#ff5454'\n",
    "COLOR_MAX = '#a43dff'\n",
    "\n",
    "#depending on the weight we assign graded colors\n",
    "def weight_to_color(w):\n",
    "    if(w < mean - std):\n",
    "        return COLOR_LOW\n",
    "    elif (w < mean):\n",
    "        return COLOR_MILD\n",
    "    elif (w < mean + std):\n",
    "        return COLOR_MEDIUM\n",
    "    elif (w < (mean + 2 * std)):\n",
    "        return COLOR_HIGH\n",
    "    else:\n",
    "        return COLOR_MAX\n",
    "\n",
    "\n",
    "for index, row in grid5.iterrows():\n",
    "    lat = np.asscalar(row['lat'])\n",
    "    lon = np.asscalar(row['lon'])\n",
    "    loc = [lat,lon]\n",
    "    if(not math.isnan(lon) and not math.isnan(lat)):\n",
    "        weight = np.asscalar(row[weight_feature])\n",
    "        color = weight_to_color(weight)\n",
    "        address = get_address(loc)\n",
    "        folium.RegularPolygonMarker(loc, popup=address, fill_color=color, rotation = -45, number_of_sides=4, radius=10, fill_opacity = 0.66).add_to(map_sd5)\n",
    "\n",
    "map_sd5.save(\"pm_late.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weight_feature = 'pm_peak'#change this for different timeslots\n",
    "\n",
    "\n",
    "#stats to use for classification\n",
    "mean = grid6[weight_feature].astype(float).mean()\n",
    "std = grid6[weight_feature].astype(float).std()\n",
    "\n",
    "#convert to floats for folium to read these\n",
    "grid6['lat'] = grid6['lat'].astype(float)\n",
    "grid6['lon'] = grid6['lon'].astype(float)\n",
    "\n",
    "#color constants for viz\n",
    "COLOR_LOW = '#50f442'\n",
    "COLOR_MILD = '#fff375'\n",
    "COLOR_MEDIUM = '#ffa154'\n",
    "COLOR_HIGH = '#ff5454'\n",
    "COLOR_MAX = '#a43dff'\n",
    "\n",
    "#depending on the weight we assign graded colors\n",
    "def weight_to_color(w):\n",
    "    if(w < mean - std):\n",
    "        return COLOR_LOW\n",
    "    elif (w < mean):\n",
    "        return COLOR_MILD\n",
    "    elif (w < mean + std):\n",
    "        return COLOR_MEDIUM\n",
    "    elif (w < (mean + 2 * std)):\n",
    "        return COLOR_HIGH\n",
    "    else:\n",
    "        return COLOR_MAX\n",
    "\n",
    "\n",
    "for index, row in grid6.iterrows():\n",
    "    lat = np.asscalar(row['lat'])\n",
    "    lon = np.asscalar(row['lon'])\n",
    "    loc = [lat,lon]\n",
    "    if(not math.isnan(lon) and not math.isnan(lat)):\n",
    "        weight = np.asscalar(row[weight_feature])\n",
    "        color = weight_to_color(weight)\n",
    "        address = get_address(loc)\n",
    "        folium.RegularPolygonMarker(loc, popup=address, fill_color=color, rotation = -45, number_of_sides=4, radius=10, fill_opacity = 0.66).add_to(map_sd6)\n",
    "\n",
    "map_sd6.save(\"pm_peak.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purple = Very High\n",
    "Red = High\n",
    "Orange = Medium\n",
    "Yellow = Low\n",
    "\"\"\"\n",
    "\n",
    "map_sd #Map for Total counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_sd2 #Map for am early"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_sd3 #Map for am peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_sd4 #Map for midday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "map_sd5 #Map for pm late"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "map_sd6 #Map for pm peak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions/Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
